# awesome-dit-accelerations

A list of awesome dit projects and papers for dit accelerations.

## Contents
- [Open Source Projects](#section1)
- [Papers](#section2)
  - Survey
  - [Quantization](#section2_quantization)
- [Tutorials & Blogs](#section3)

## Open Source Projects<a id="section1"></a>
- [xDiT: an Inference Engine for Diffusion Transformers (DiTs) with Massive Parallelism]([https://arxiv.org/abs/2411.01738](https://github.com/xdit-project/xDiT))
- [USP: A Unified Sequence Parallelism Approach for Long Context Generative AI](https://github.com/feifeibear/long-context-attention) by Tencent

## Papers<a id="section2"></a>

### Survey
- [Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study](https://arxiv.org/abs/2411.13588) by Tencent, arXiv 2024

### Quantization<a id="section2_quantization"></a>
- [AKVQ-VL: Attention-Aware KV Cache Adaptive
2-Bit Quantization for Vision-Language Models](https://arxiv.org/abs/2501.15021) by Su, Zunhai, et al., arXiv 2025
- [PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](https://arxiv.org/abs/2505.18610) by Liu, Tengxuan, et al., arXiv 2025
- [ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification](https://arxiv.org/abs/2405.14256) by He, Yefei, et al., NeurIPS 2025
- [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054) by Zhao, Tianchen, et al., arXiv 2025
- [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2410.02367) by Zhang, Jintao, et al., ICLR 2025
- [SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization](https://arxiv.org/abs/2411.10958) by Zhang, Jintao, et al., ICML 2025
- [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594) by Zhang, Jintao, et al., NeurIPS 2025

## Tutorials & Blogs<a id="section3"></a>
