# awesome-dit-accelerations

A list of awesome dit projects and papers for dit accelerations.

## Contents
- [Open Source Projects](#section1)
- [Papers](#section2)
  - Survey
  - [Quantization](#section2_quantization)
- [Tutorials & Blogs](#section3)

## Open Source Projects<a id="section1"></a>

## Papers<a id="section2"></a>

### Survey

### Quantization<a id="section2_quantization"></a>
- [AKVQ-VL: Attention-Aware KV Cache Adaptive
2-Bit Quantization for Vision-Language Models](https://arxiv.org/abs/2501.15021) by Su, Zunhai, et al., arXiv 2025
- [PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](https://arxiv.org/abs/2505.18610) by Liu, Tengxuan, et al., arXiv 2025
- [ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification](https://arxiv.org/abs/2405.14256) by He, Yefei, et al., NeurIPS 2025
- [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054) by Zhao, Tianchen, et al., arXiv 2025

## Tutorials & Blogs<a id="section3"></a>
