# awesome-dit-accelerations

A list of awesome dit projects and papers for dit accelerations.

## Contents
- [Open Source Projects](#section1)
  - [Video Generation Models](#section1_video_generation_models)
  - [DiT Accelerations](#section1_dit_accelerations)
- [Papers](#section2)
  - [Survey](#section2_survey)
  - [AR-Diffusion](#section2_ar_diffusion)
  - [Quantization](#section2_quantization)
- [Tutorials & Blogs](#section3)

## Open Source Projects<a id="section1"></a>

### Video Generation Models<a id="section1_video_generation_models"></a>
- [Mochi 1](https://www.genmo.ai/blog/mochi-1-a-new-sota-in-open-text-to-video) by Genmo
- [Wan](https://wan.video/) by Alibaba
- [Hunyuan](https://hunyuan.tencent.com/) by Tencent
- [CogVideoX](https://github.com/zai-org/CogVideo) by Zhipu AI
- [Open-Sora-Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan) by PKU
- [Open-Sora](https://github.com/hpcaitech/Open-Sora) by Colossal-AI

### DiT Accelerations<a id="section1_dit_accelerations"></a>
- [xDiT: an Inference Engine for Diffusion Transformers (DiTs) with Massive Parallelism](https://github.com/xdit-project/xDiT)
- [USP: A Unified Sequence Parallelism Approach for Long Context Generative AI](https://github.com/feifeibear/long-context-attention) by Tencent

## Papers<a id="section2"></a>

### Survey<a id="section2_survey"></a>
- [Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study](https://arxiv.org/abs/2411.13588) by Tencent, arXiv 2024

### AR-Diffusion<a id="section2_ar_diffusion"></a>
- [TiDAR: Think in Diffusion, Talk in Autoregression](https://arxiv.org/abs/2511.08923) by NVIDIA, arXiv 2025
- [AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion](https://arxiv.org/abs/2503.07418) by Bytedance Inc., CVPR 2025

### Quantization<a id="section2_quantization"></a>
- [AKVQ-VL: Attention-Aware KV Cache Adaptive
2-Bit Quantization for Vision-Language Models](https://arxiv.org/abs/2501.15021) by Su, Zunhai, et al., arXiv 2025
- [PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](https://arxiv.org/abs/2505.18610) by Liu, Tengxuan, et al., arXiv 2025
- [ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification](https://arxiv.org/abs/2405.14256) by He, Yefei, et al., NeurIPS 2025
- [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054) by Zhao, Tianchen, et al., arXiv 2025
- [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2410.02367) by Zhang, Jintao, et al., ICLR 2025
- [SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization](https://arxiv.org/abs/2411.10958) by Zhang, Jintao, et al., ICML 2025
- [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594) by Zhang, Jintao, et al., NeurIPS 2025

## Tutorials & Blogs<a id="section3"></a>
